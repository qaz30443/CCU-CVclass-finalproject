{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3479bb84-f2ac-4843-93ce-821ba46ea40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "\n",
    "warnings.simplefilter(\"ignore\", (UserWarning, FutureWarning))\n",
    "from utils.hparams import HParam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "from utils import metrics\n",
    "from core.res_unet import ResUnet\n",
    "from core.res_unet_plus import ResUnetPlusPlus\n",
    "from core.unet import UNetSmall\n",
    "from utils.logger import MyWriter\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from dataset import *\n",
    "\n",
    "from __future__ import print_function, division\n",
    "from typing import Any\n",
    "from skimage import io\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc9061e-471c-4d46-994c-6110072a7ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path =  \"../CV_class/training/mask*.png\"\n",
    "# valid_path =  \"../CV_class/testing/mask*.png\"\n",
    "log =  \"logs\"\n",
    "logging_step = 100\n",
    "validation_interval = 7000 # Save and valid have same interval\n",
    "checkpoints = \"checkpoints\"\n",
    "\n",
    "batch_size = 2\n",
    "lr = 0.001\n",
    "RESNET_PLUS_PLUS = True\n",
    "IMAGE_SIZE = 1600\n",
    "CROP_SIZE = 224\n",
    "\n",
    "epochs = 20\n",
    "resume = \"\"\n",
    "name = \"default\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c48860b1-ce86-436e-ba09-d1d6a26830c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_image(tensordata_input, tensordata_label):\n",
    "    \n",
    "    numpy_input = tensordata_input.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    numpy_label = tensordata_label.permute(1, 2, 0).cpu().detach().numpy()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8, 4),\n",
    "                                    sharex=True, sharey=True)\n",
    "    \n",
    "    ax1.imshow(numpy_input)\n",
    "    ax2.imshow(numpy_label)\n",
    "    \n",
    "    for aa in (ax1, ax2):\n",
    "        aa.set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ee39f07-5dcd-47a5-8299-c000d2eb8571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = \"{}/{}\".format(checkpoints, name)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(\"{}/{}\".format(log, name), exist_ok=True)\n",
    "writer = MyWriter(\"{}/{}\".format(log, name))\n",
    "\n",
    "\n",
    "# get model\n",
    "print(RESNET_PLUS_PLUS)\n",
    "if RESNET_PLUS_PLUS:\n",
    "    model = ResUnetPlusPlus(3).to(device)\n",
    "else:\n",
    "    model = UNetSmall(3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b7e7d05-386b-4617-84b9-4b6c384d2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally resume from a checkpoint\n",
    "if resume:\n",
    "    if os.path.isfile(resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "        checkpoint = torch.load(resume)\n",
    "\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "        best_loss = checkpoint[\"best_loss\"]\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        print(\n",
    "            \"=> loaded checkpoint '{}' (epoch {})\".format(\n",
    "                resume, checkpoint[\"epoch\"]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d632d0d7-e2ef-443a-94d6-331d8ab3585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfm = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((CROP_SIZE, CROP_SIZE)),\n",
    "    transforms.ToTensor(), \n",
    "])\n",
    "\n",
    "# get data\n",
    "mass_dataset_train = ImageDataset(\n",
    "    transform=test_tfm\n",
    ")\n",
    "\n",
    "mass_dataset_val = ImageDataset(\n",
    "    False, transform=test_tfm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# creating loaders\n",
    "train_dataloader = DataLoader(\n",
    "    mass_dataset_train, batch_size=batch_size, shuffle=True, pin_memory=False\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    mass_dataset_val, batch_size=1, shuffle=False, pin_memory=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f652125-b91d-4b15-9908-a9d19d4c231c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.5222 Acc: 0.6799: 100%|████| 1452/1452 [03:43<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2736 Acc: 0.7995: 100%|████| 1452/1452 [03:39<00:00,  6.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.2183 Acc: 0.8341: 100%|████| 1452/1452 [03:41<00:00,  6.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1910 Acc: 0.8512: 100%|████| 1452/1452 [03:38<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1792 Acc: 0.8583: 100%|████| 1452/1452 [03:36<00:00,  6.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1621 Acc: 0.8709: 100%|████| 1452/1452 [03:37<00:00,  6.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1496 Acc: 0.8787: 100%|████| 1452/1452 [03:38<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1406 Acc: 0.8850: 100%|████| 1452/1452 [03:39<00:00,  6.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1345 Acc: 0.8877: 100%|████| 1452/1452 [03:38<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1231 Acc: 0.8956: 100%|████| 1452/1452 [03:38<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1183 Acc: 0.8982: 100%|████| 1452/1452 [03:38<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1153 Acc: 0.9006: 100%|████| 1452/1452 [03:38<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1118 Acc: 0.9024: 100%|████| 1452/1452 [03:37<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1043 Acc: 0.9088: 100%|████| 1452/1452 [03:38<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.1041 Acc: 0.9090: 100%|████| 1452/1452 [03:37<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0999 Acc: 0.9134: 100%|████| 1452/1452 [03:40<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0987 Acc: 0.9131: 100%|████| 1452/1452 [03:39<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0929 Acc: 0.9186: 100%|████| 1452/1452 [03:38<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0917 Acc: 0.9193: 100%|████| 1452/1452 [03:35<00:00,  6.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/19\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0812 Acc: 0.9280: 100%|████| 1452/1452 [03:36<00:00,  6.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# set up binary cross entropy and dice loss\n",
    "criterion = metrics.BCEDiceLoss()\n",
    "\n",
    "# optimizer\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# decay LR\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# starting params\n",
    "best_loss = 999\n",
    "start_epoch = 0\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "step = 0\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "    print(\"Epoch {}/{}\".format(epoch, epochs - 1))\n",
    "    print(\"-\" * 10)\n",
    "\n",
    "    # step the learning rate scheduler\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # run training and validation\n",
    "    # logging accuracy and loss\n",
    "    train_acc = metrics.MetricTracker()\n",
    "    train_loss = metrics.MetricTracker()\n",
    "    # iterate over data\n",
    "\n",
    "    loader = tqdm(train_dataloader, desc=\"training\")\n",
    "    for idx, data in enumerate(loader):\n",
    "        # get the inputs and wrap in Variable\n",
    "        inputs = data[\"sat_img\"].to(device)\n",
    "        labels = data[\"map_img\"].to(device)\n",
    "        # ====test=====\n",
    "        # show_tensor_image(inputs[0], labels[0])\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        # prob_map = model(inputs) # last activation was a sigmoid\n",
    "        # outputs = (prob_map > 0.3).float()\n",
    "        outputs = model(inputs)\n",
    "        # outputs = torch.nn.functional.sigmoid(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc.update(metrics.dice_coeff(outputs, labels), outputs.size(0))\n",
    "        train_loss.update(loss.data.item(), outputs.size(0))\n",
    "\n",
    "        # tensorboard logging\n",
    "        if step % logging_step == 0:\n",
    "            writer.log_training(train_loss.avg, train_acc.avg, step)\n",
    "            loader.set_description(\n",
    "                \"Training Loss: {:.4f} Acc: {:.4f}\".format(\n",
    "                    train_loss.avg, train_acc.avg\n",
    "                )\n",
    "            )\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96992780-7db8-4e38-b399-7e741cec399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing: 100%|████████████████████████████████| 200/200 [00:06<00:00, 31.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss: 2.9300 Acc: 0.7992\n",
      "Saved checkpoint to: checkpoints/default/default_checkpoint_29040.pt\n"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "\n",
    "# logging accuracy and loss\n",
    "valid_acc = metrics.MetricTracker()\n",
    "valid_loss = metrics.MetricTracker()\n",
    "\n",
    "# switch to evaluate mode\n",
    "model.eval()\n",
    "\n",
    "# Iterate over data.\n",
    "for idx, data in enumerate(tqdm(val_dataloader, desc=\"testing\")):\n",
    "\n",
    "    # get the inputs and wrap in Variable\n",
    "    inputs = data[\"sat_img\"].to(device)\n",
    "    labels = data[\"map_img\"].to(device)\n",
    "    \n",
    "    # forward\n",
    "    # prob_map = model(inputs) # last activation was a sigmoid\n",
    "    # outputs = (prob_map > 0.3).float()\n",
    "    outputs = model(inputs)\n",
    "    # outputs = torch.nn.functional.sigmoid(outputs)\n",
    "    # ======= test =======\n",
    "    # show_tensor_image(outputs[0], labels[0])\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    valid_acc.update(metrics.dice_coeff(outputs, labels), outputs.size(0))\n",
    "    valid_loss.update(loss.data.item(), outputs.size(0))\n",
    "    # if idx == 0:\n",
    "    #     writer.log_images(inputs.to(device), labels.to(device), outputs.to(device), step)\n",
    "writer.log_validation(valid_loss.avg, valid_acc.avg, step)\n",
    "\n",
    "print(\"Testing Loss: {:.4f} Acc: {:.4f}\".format(valid_loss.avg, valid_acc.avg))\n",
    "model.train()\n",
    "valid_metrics = {\"valid_loss\": valid_loss.avg, \"valid_acc\": valid_acc.avg}\n",
    "save_path = os.path.join(\n",
    "    checkpoint_dir, \"%s_checkpoint_%04d.pt\" % (name, step)\n",
    ")\n",
    "# store best loss and save a model checkpoint\n",
    "best_loss = min(valid_metrics[\"valid_loss\"], best_loss)\n",
    "torch.save(\n",
    "    {\n",
    "        \"step\": step,\n",
    "        \"epoch\": epoch,\n",
    "        \"arch\": \"ResUnet\",\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"best_loss\": best_loss,\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    },\n",
    "    save_path,\n",
    ")\n",
    "print(\"Saved checkpoint to: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d825f3f-d4de-4679-acb7-5c2daa04b374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
